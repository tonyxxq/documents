## MC(蒙特卡洛)

蒙特卡洛方法的整体思路是：**模拟 -> 抽样 -> 估值**

1. 策略评估迭代

   > 每一个状态的每一个动作都进行了大量的抽样去判断在该状态下该行动价值的大小

   - 探索 - 选择一个状态 (s, a)
   - 模拟 - 使用当前策略 π，进行一次模拟，从当前状态 (s, a) 到结束，随机产生一段情节 (episode)
   - 抽样 - 获得这段情节上的每个状态 (s, a) 的回报 G(s, a) ，记录 G(s, a) 到集合 Returns(s, a)，（**注意：这个地方的每个状态的每个动作可能会多次出现，最后判断其价值，是使用求平均值的方式**）
   - 估值 - q(s, a) = Returns(s, a)的平均值

2. 策略优化

   使用新的行动价值 q(s, a) 优化策略 π(s)

强化学习中不同的蒙特卡洛方法：

- 蒙特卡洛（起始点（Exploring Starts））方法
- On-policy first visit 蒙特卡洛方法（for ϵϵ-soft policies）
- Off-policy every-visit 蒙特卡洛方法

蒙特卡洛法和动态规划的区别：

动态规划是从初始状态开始，一次计算一步可能发生的所有状态价值，然后迭代计算下一步的所有状态价值。这就是引导性。
蒙特卡洛方法是从初始状态开始，通过在实际环境中模拟，得到一段情节（从头到结束）。比如，如果结束是失败了，这段情节上的状态节点，本次价值都为0,；如果成功了，本次价值都为1



参考：https://www.cnblogs.com/steven-yang/p/6507015.html